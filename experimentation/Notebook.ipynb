{
 "metadata": {
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "interpreter": {
   "hash": "50b37eb35d55879b46f571aef5e85bb73790ee1370826879b0c77563c93e6dfb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'helpers'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-78fe5181e152>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhelpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mflatten_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbuild_graphml_file\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_user_movie_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_movie_prop_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'helpers'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from helpers import flatten_list\n",
    "from build_graphml_file import get_user_movie_graph, get_movie_prop_graph\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_tag_vec = pd.read_csv(\"cleaned/genome_tag_vec.csv\")\n",
    "imdb_data = pd.read_csv(\"cleaned/imdb_data.csv\")\n",
    "movies = pd.read_csv(\"cleaned/movies.csv\")\n",
    "# tags = pd.read_csv(\"data/tags.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "train = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_tag_vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distrib = train.rating.value_counts(normalize=True)\n",
    "\n",
    "fig = px.bar(distrib, color=distrib.index)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, Normalizer, StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_combine = movies.merge(imdb_data, on='movieId', how='left')\n",
    "normal_year_mean = full_combine[full_combine.year != 0].year.mean()\n",
    "full_combine['year'].replace(0, normal_year_mean, inplace=True)\n",
    "\n",
    "# movies.year.fillna(int(movies.year.median()), inplace=True)\n",
    "# movies.genres.fillna(\"<unknown>\", inplace=True)\n",
    "full_combine.title = full_combine.title.str.strip().str.replace(r\"(.*), The$\", r\"The \\1\", regex=True)\n",
    "full_combine.title_cast.fillna(\"\", inplace=True)\n",
    "full_combine.director.fillna(\"\", inplace=True)\n",
    "full_combine.runtime.fillna(int(full_combine.runtime.median()), inplace=True)\n",
    "full_combine.budget.fillna(int(full_combine.budget.median()), inplace=True)\n",
    "full_combine.plot_keywords.fillna(\"\", inplace=True)\n",
    "\n",
    "full_combine['cast_size'] = full_combine.title_cast.str.split('|').apply(len)\n",
    "full_combine['genre_count'] = full_combine.genres.str.split('|').apply(len)\n",
    "\n",
    "movie_groups = train.groupby('movieId')\n",
    "full_combine['rating_mean'] = movie_groups.rating.mean()\n",
    "full_combine['rating_std'] = movie_groups.rating.std()\n",
    "full_combine['rating_iqr'] =  movie_groups.rating.quantile(0.75) - movie_groups.rating.quantile(0.25)\n",
    "full_combine['rating_count'] = movie_groups.rating.count()\n",
    "\n",
    "full_combine.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "full_combine.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title_vectrz = TfidfVectorizer(min_df=20, ngram_range=(1, 3), stop_words=stopwords, norm=None)\n",
    "# title_vec = title_vectrz.fit_transform(full_combine.title)\n",
    "# print(\"Title Tokens:\", len(title_vectrz.get_feature_names()))\n",
    "# features.extend(title_vectrz.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_vectrz = TfidfVectorizer(token_pattern=r\"[A-z\\-]+\", min_df=2, norm=None)\n",
    "genre_vec = genre_vectrz.fit_transform(full_combine.genres)\n",
    "print(\"Genre Tokens:\", len(genre_vectrz.get_feature_names()))\n",
    "features.extend(genre_vectrz.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast_vectrz = TfidfVectorizer(token_pattern=r\"[^\\|]+\", min_df=50, norm=None)\n",
    "# cast_vec = cast_vectrz.fit_transform(full_combine.title_cast)\n",
    "# print(\"Cast Tokens:\", len(cast_vectrz.get_feature_names()))\n",
    "# features.extend(cast_vectrz.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# director_vectrz = TfidfVectorizer(token_pattern=r\".+\", min_df=5, stop_words=['see full summary'], norm=None)\n",
    "# director_vec = director_vectrz.fit_transform(full_combine.director)\n",
    "# print(\"Director Tokens:\", len(director_vectrz.get_feature_names()))\n",
    "# features.extend(director_vectrz.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vectrz = TfidfVectorizer(token_pattern=r\"[^\\|]+\", min_df=20, stop_words=stopwords, norm=None)\n",
    "plot_vec = plot_vectrz.fit_transform(full_combine.plot_keywords)\n",
    "print(\"Plot KW Tokens:\", len(plot_vectrz.get_feature_names()))\n",
    "features.extend(plot_vectrz.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gtag_vec = genome_tag_vec.drop('movieId', axis=1).values\n",
    "# gtag_vec_sm = sparse.coo_matrix(gtag_vec, shape=(10000,1128))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_features = [\"year\", 'rating_mean', 'rating_std', 'rating_iqr', 'rating_count']\n",
    "features.extend(extra_features)\n",
    "\n",
    "extra_features = full_combine[extra_features]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "transformed = scaler.fit_transform(extra_features)\n",
    "std_extra_sparse = sparse.csr_matrix(transformed)\n",
    "\n",
    "tfidf_vecs = sparse.hstack([\n",
    "    # title_vec,\n",
    "    genre_vec,\n",
    "    # cast_vec,\n",
    "    # director_vec,\n",
    "    plot_vec,\n",
    "]).tocsr()\n",
    "\n",
    "vecs = sparse.hstack([tfidf_vecs, std_extra_sparse])\n",
    "norm = Normalizer(copy=True)\n",
    "norm_vecs = norm.transform(vecs)\n",
    "norm_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = norm_vecs.getrow(17067).toarray().flatten()\n",
    "b = norm_vecs.getrow(14628).toarray().flatten()\n",
    "\n",
    "f = pd.DataFrame(index=features)\n",
    "f['a'] = a\n",
    "f['b'] = b\n",
    "f['diff'] = np.abs(f.a - f.b)\n",
    "\n",
    "f[f.a != f.b].sort_values('diff', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_to(movieId):\n",
    "    movie_idx = movies[movies['movieId'] == movieId].index[0]\n",
    "\n",
    "    return norm_vecs.dot(norm_vecs.getrow(movie_idx).transpose())\n",
    "\n",
    "avengers = 89745\n",
    "avengers2 = 122892\n",
    "amazing_spiderman = 95510\n",
    "fault_in_stars = 111921\n",
    "\n",
    "mst = most_similar_to(avengers).toarray().flatten()\n",
    "n = 10\n",
    "top_n = mst.argsort(axis=0)[-n:][::-1]\n",
    "bot_n = mst.argsort(axis=0)[:n][::-1]\n",
    "\n",
    "for similar_idx in top_n:\n",
    "    title = full_combine.iloc[similar_idx].title\n",
    "    sim = round(mst[similar_idx], 10)\n",
    "    print(sim, similar_idx, title)\n",
    "\n",
    "print('-' * 20)\n",
    "\n",
    "for similar_idx in bot_n:\n",
    "    title = full_combine.iloc[similar_idx].title\n",
    "    sim = mst[similar_idx]\n",
    "    print(sim, title)\n",
    "# most_similar = most_similar_to(avengers).toarray().argmax()\n",
    "# movie_data.iloc[most_similar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "movie_indexes = full_combine.movieId.copy()\n",
    "experiment = train.copy().drop('timestamp', axis=1)\n",
    "\n",
    "mid_to_idx = pd.Series(movie_indexes.index.values, index=movie_indexes).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(id1, id2):\n",
    "    a = vecs[mid_to_idx[id1], :]\n",
    "    b = vecs[mid_to_idx[id2], :]\n",
    "    return a.dot(b.transpose()).toarray().flatten()[0]\n",
    "\n",
    "def batch_similarity(id1, id_list: list):\n",
    "    a = vecs[mid_to_idx[id1], :]\n",
    "    idxs = [mid_to_idx[i] for i in id_list]\n",
    "    bs = vecs[idxs, :]\n",
    "    return bs.dot(a.transpose()).toarray().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _test = train.groupby('userId').sample(frac=0.25)\n",
    "# _train = train[~train.index.isin(_test.index.values)]\n",
    "def skip_diag_strided(A):\n",
    "    m = A.shape[0]\n",
    "    strided = np.lib.stride_tricks.as_strided\n",
    "    s0,s1 = A.strides\n",
    "    return strided(A.ravel()[1:], shape=(m-1,m), strides=(s0+s1,s1)).reshape(m,-1)\n",
    "\n",
    "def remove_diag(x):\n",
    "    x_no_diag = np.ndarray.flatten(x)\n",
    "    x_no_diag = np.delete(x_no_diag, range(0, len(x_no_diag), len(x) + 1), 0)\n",
    "    x_no_diag = x_no_diag.reshape(len(x), len(x) - 1)\n",
    "    return x_no_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_movie_rating_by_movie_id = train.groupby('movieId').rating.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.concat([train, test])\n",
    "combined_data.drop(\"timestamp\", axis=1, inplace=True)\n",
    "combined_data.sort_values(['userId', 'rating'], inplace=True)\n",
    "combined_data.reset_index(inplace=True, drop=True)\n",
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def predict_ratings(data):\n",
    "    history = data[data.rating.notna()]\n",
    "    to_predict = data[data.rating.isna()]\n",
    "\n",
    "    if len(history) == 1:\n",
    "        print(data.userId.unique())\n",
    "        mean_guesses = mean_movie_rating_by_movie_id.loc[to_predict.movieId.values].values\n",
    "        print(mean_guesses)\n",
    "        to_predict['pred'] = mean_guesses\n",
    "        return pd.concat([history, to_predict])\n",
    "\n",
    "    if len(to_predict) == 0:\n",
    "        return data\n",
    "\n",
    "    idxs = [mid_to_idx[i] for i in data.movieId]\n",
    "\n",
    "    historic_ratings = history.rating.values\n",
    "\n",
    "    similarity_matrix = remove_diag(cosine_similarity(norm_vecs[idxs, :]))\n",
    "    sim_matrix = similarity_matrix[len(history):, :len(history)]\n",
    "    \n",
    "    sim_totals = sim_matrix.sum(axis=1)\n",
    "    ratings = np.tile(history.rating, (len(to_predict.rating),1))\n",
    "    weighted_sums = np.einsum('ij,ij->i', sim_matrix, ratings)\n",
    "\n",
    "    to_predict['pred'] = weighted_sums / sim_totals\n",
    "    \n",
    "    return pd.concat([history, to_predict])\n",
    "\n",
    "predictions = combined_data.groupby('userId').apply(predict_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[predictions.userId == 53640]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = predictions.copy()\n",
    "submission.drop(submission[submission.rating.notna()].index, inplace=True)\n",
    "submission[\"rating\"] = submission[\"pred\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['Id'] = submission.userId.astype(str) + '_' + submission.movieId.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "submission[['Id', 'rating']].to_csv(\"submission.csv\", index=False, chunksize=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}